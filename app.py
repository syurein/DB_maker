import os
import json
import time
import requests
import pandas as pd
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from openai import OpenAI
from playwright.sync_api import sync_playwright
import gradio as gr
from urllib.parse import quote

# è¨­å®šèª­ã¿è¾¼ã¿
load_dotenv()

# --- å®šæ•°ãƒ»è¨­å®š ---
IMAGE_DIR = "downloaded_images"
os.makedirs(IMAGE_DIR, exist_ok=True)
SELECTORS_PATH = "selectors.json"

# AIè¨­å®š
DEFAULT_API_KEY = os.getenv("DEEPSEEK_API_KEY")
DEFAULT_BASE_URL = os.getenv("DEEPSEEK_BASE_URL", "https://api.openai.com/v1")
DEFAULT_MODEL = os.getenv("AI_MODEL", "gpt-4o-mini")
HEADLESS_MODE = os.getenv("HEADLESS_MODE", "True")

# ã‚«ãƒ†ã‚´ãƒªãƒãƒƒãƒ—èª­ã¿è¾¼ã¿
CATEGORY_CSV_PATH = "ãƒ¡ãƒ«ã‚«ãƒªã‚«ãƒ†ã‚´ãƒªä¸€è¦§.csv"

def load_category_map(csv_path):
    cat_map = {}
    if os.path.exists(csv_path):
        try:
            try:
                df = pd.read_csv(csv_path, header=None, encoding='utf-8')
            except:
                df = pd.read_csv(csv_path, header=None, encoding='cp932')
            for _, row in df.iterrows():
                if pd.notna(row[1]): cat_map[str(row[1]).strip()] = int(row[0])
                if len(row) > 3 and pd.notna(row[3]): cat_map[f"{row[1]} > {row[3]}"] = int(row[2])
        except: pass
    return cat_map

CATEGORY_MAP = load_category_map(CATEGORY_CSV_PATH)
CATEGORY_CHOICES = list(CATEGORY_MAP.keys()) if CATEGORY_MAP else []

# --- ã‚»ãƒ¬ã‚¯ã‚¿ç®¡ç†ã‚¯ãƒ©ã‚¹ ---
class SelectorManager:
    def __init__(self):
        self.selectors = self._load()
        # åˆæœŸå€¤ãŒãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’è¨­å®š
        updated = False
        if "item_container" not in self.selectors:
            self.selectors["item_container"] = ["li[data-testid='item-cell']", "div[data-testid='item-cell']"]
            updated = True
        if "title" not in self.selectors:
            self.selectors["title"] = ["img[alt]", "[data-testid='thumbnail-image']"]
            updated = True
        if "price" not in self.selectors:
            self.selectors["price"] = [".number__6b270ca7", "[data-testid='price']"]
            updated = True
        
        # åˆæœŸå€¤ã‚’å…¥ã‚ŒãŸå ´åˆã‚‚å³ä¿å­˜
        if updated:
            self.save()

    def _load(self):
        if os.path.exists(SELECTORS_PATH):
            try:
                with open(SELECTORS_PATH, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except: pass
        return {}

    def save(self):
        """ç¾åœ¨ã®ã‚»ãƒ¬ã‚¯ã‚¿æƒ…å ±ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚€"""
        try:
            with open(SELECTORS_PATH, 'w', encoding='utf-8') as f:
                json.dump(self.selectors, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ ã‚»ãƒ¬ã‚¯ã‚¿è¨­å®šã‚’JSONã«ä¿å­˜ã—ã¾ã—ãŸ: {SELECTORS_PATH}")
        except Exception as e:
            print(f"âŒ JSONä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def get_candidates(self, key):
        val = self.selectors.get(key, [])
        if isinstance(val, str): return [val]
        return val

    def add_prioritized(self, key, new_selector):
        """æ–°ã—ã„ã‚»ãƒ¬ã‚¯ã‚¿ã‚’ãƒªã‚¹ãƒˆã®å…ˆé ­ã«è¿½åŠ ã—ã¦ä¿å­˜ã™ã‚‹"""
        print(f"ğŸ”„ ã‚»ãƒ¬ã‚¯ã‚¿æ›´æ–°ãƒ»å„ªå…ˆé †ä½å¤‰æ›´: {key} -> {new_selector}")
        current = self.get_candidates(key)
        # é‡è¤‡ã‚’é™¤ãã¤ã¤ã€æ–°ã—ã„ã‚»ãƒ¬ã‚¯ã‚¿ã‚’å…ˆé ­ã«è¿½åŠ 
        new_list = [new_selector] + [x for x in current if x != new_selector]
        self.selectors[key] = new_list
        # â˜…ã“ã“ã§ä¿å­˜ã‚’å®Ÿè¡Œ
        self.save()

# --- ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼æœ¬ä½“ ---
class MercariSmartScraper:
    def __init__(self, api_key, base_url, model_name):
        self.selector_manager = SelectorManager()
        self.api_key = api_key
        self.base_url = base_url
        self.model_name = model_name
        self.client = None
        if self.api_key:
            self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)

    def _clean_html(self, html_content):
        soup = BeautifulSoup(html_content, 'html.parser')
        for tag in soup(['script', 'style', 'svg', 'path', 'noscript', 'iframe', 'meta', 'link']):
            tag.decompose()
        return str(soup)[:30000]

    def _ask_ai_for_selector(self, html_snippet, target_description, failed_selectors=None):
        if not self.client:
            print("âš ï¸ API KeyãŒãªã„ãŸã‚AIä¿®å¾©ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return None

        print(f"ğŸš‘ AI Healingãƒªã‚¯ã‚¨ã‚¹ãƒˆ: {target_description}")
        
        system_prompt = "ã‚ãªãŸã¯Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã®å°‚é–€å®¶ã§ã™ã€‚CSSã‚»ãƒ¬ã‚¯ã‚¿ã®ã¿ã‚’JSONå½¢å¼ã§è¿”ã—ã¦ãã ã•ã„ã€‚"
        user_prompt = f"""
        ä»¥ä¸‹ã®HTMLã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ã€Œ{target_description}ã€ã‚’ç‰¹å®šã™ã‚‹CSSã‚»ãƒ¬ã‚¯ã‚¿ã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚
        
        ã€é™¤å¤–ãƒªã‚¹ãƒˆã€‘
        {json.dumps(failed_selectors)}

        ã€æ¡ä»¶ã€‘
        - å®‰å®šã—ãŸå±æ€§ï¼ˆdata-testid, aria-labelç­‰ï¼‰ã‚’å„ªå…ˆã€‚
        - ãªã‘ã‚Œã°classå±æ€§ãªã©ã‚’ä½¿ç”¨ã€‚
        - 1ã¤ã ã‘ææ¡ˆã—ã¦ãã ã•ã„ã€‚

        ã€HTMLã€‘
        {html_snippet}

        ã€å‡ºåŠ›å½¢å¼ (JSON)ã€‘
        {{"selector": "ã‚ãªãŸã®ç­”ãˆ"}}
        """

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                response_format={"type": "json_object"},
                temperature=0.0
            )
            content = response.choices[0].message.content
            cleaned = content.replace("```json", "").replace("```", "").strip()
            result = json.loads(cleaned)
            new_selector = result.get("selector")
            print(f"âœ¨ AIææ¡ˆ: {new_selector}")
            return new_selector
        except Exception as e:
            print(f"âŒ AI Error: {e}")
            return None

    def _find_elements_with_healing(self, page, key, description):
        candidates = self.selector_manager.get_candidates(key)
        
        # 1. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚ŒãŸå€™è£œã‚’è©¦ã™
        for sel in candidates:
            try:
                count = page.locator(sel).count()
                if count > 0:
                    return page.locator(sel).all()
            except: pass
        
        # 2. å…¨å€™è£œè©¦ã—ã¦å…¨éƒ¨0ä»¶ -> AI Healingç™ºå‹•
        print(f"âš ï¸ {key} ã®æ—¢å­˜ã‚»ãƒ¬ã‚¯ã‚¿ã§ã¯0ä»¶ã§ã—ãŸã€‚AIä¿®å¾©ã‚’å®Ÿè¡Œã—ã¾ã™...")
        
        try:
            html_content = page.content()
            if not html_content or len(html_content) < 100:
                print("âŒ ãƒšãƒ¼ã‚¸ã®å†…å®¹ãŒç©ºã§ã™ã€‚ä¿®å¾©ã§ãã¾ã›ã‚“ã€‚")
                return []
        except: return []

        html_context = self._clean_html(html_content)
        new_sel = self._ask_ai_for_selector(html_context, description, candidates)
        
        if new_sel:
            # â˜…ã“ã“ã§æ–°ã—ã„ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è¿½åŠ ï¼†ä¿å­˜
            self.selector_manager.add_prioritized(key, new_sel)
            try:
                count = page.locator(new_sel).count()
                if count > 0:
                    print(f"âœ… ä¿®å¾©æˆåŠŸï¼ {count}ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
                    return page.locator(new_sel).all()
            except: pass
            
        return []

    def _get_text_with_healing(self, item_locator, key, description):
        candidates = self.selector_manager.get_candidates(key)
        for sel in candidates:
            try:
                target = item_locator.locator(sel).first
                if key == "title" and "img" in sel:
                    text = target.get_attribute("alt")
                else:
                    text = target.inner_text()
                if text and text.strip(): return text.strip()
            except: pass
        
        # å–å¾—å¤±æ•—æ™‚ã®Healing (ã‚¢ã‚¤ãƒ†ãƒ å˜ä½“)
        try:
            item_html = item_locator.inner_html()
            # print(f"âš ï¸ {key} ãŒç©ºã§ã—ãŸã€‚ä¿®å¾©ä¸­...") 
            new_sel = self._ask_ai_for_selector(self._clean_html(item_html), description, candidates)
            
            if new_sel:
                # â˜…ã“ã“ã§æ–°ã—ã„ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è¿½åŠ ï¼†ä¿å­˜
                self.selector_manager.add_prioritized(key, new_sel)
                try:
                    target = item_locator.locator(new_sel).first
                    if key == "title" and "img" in new_sel:
                        return target.get_attribute("alt")
                    return target.inner_text()
                except: pass
        except: pass

        return "" 

    def run(self, keyword, category_id, status, price_min, price_max, limit, progress=gr.Progress()):
        results = []
        status_param = "on_sale%7Csold_out" if status == "ã™ã¹ã¦" else ("sold_out" if status == "å£²ã‚Šåˆ‡ã‚Œ" else "on_sale")
        safe_kw = "".join([c for c in keyword if c.isalnum()])
        csv_filename = f"{safe_kw}_{limit}ä»¶.csv"
        pd.DataFrame(columns=["å•†å“å", "ä¾¡æ ¼", "ç”»åƒãƒ‘ã‚¹", "URL"]).to_csv(csv_filename, index=False, encoding="utf-8-sig")

        with sync_playwright() as p:
            browser = p.chromium.launch(headless=HEADLESS_MODE.lower() == "true")
            context = browser.new_context(user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36")
            page = context.new_page()

            count = 0
            page_idx = 0
            
            while count < limit:
                page_token = f"v1%3A{page_idx}"
                url = f"https://jp.mercari.com/search?keyword={quote(keyword)}&status={status_param}&sort=created_time&order=desc&page_token={page_token}"
                if category_id: url += f"&category_id={category_id}"
                if price_min: url += f"&price_min={price_min}"
                if price_max: url += f"&price_max={price_max}"

                print(f"ğŸŒ Accessing Page {page_idx}: {url}")
                
                try:
                    page.goto(url, timeout=30000)
                    page.wait_for_load_state("networkidle", timeout=5000)
                    time.sleep(2)
                except Exception as e:
                    if "Timeout" in str(e):
                        print("âš ï¸ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç™ºç”Ÿ: èª­ã¿è¾¼ã¿ã‚’å¼·åˆ¶åœæ­¢ã—ã€ç¾åœ¨ã®è¡¨ç¤ºçŠ¶æ…‹ã§è§£æã‚’è©¦ã¿ã¾ã™...")
                        try:
                            page.evaluate("window.stop()")
                        except: pass
                    else:
                        print(f"âŒ è‡´å‘½çš„ãªã‚¨ãƒ©ãƒ¼: {e}")
                        break

                items = self._find_elements_with_healing(
                    page, 
                    "item_container", 
                    "æ¤œç´¢çµæœä¸€è¦§ã®å€‹ã€…ã®å•†å“ã‚’å›²ã‚€ã‚³ãƒ³ãƒ†ãƒŠè¦ç´ (liã‚¿ã‚°ã‚„divã‚¿ã‚°)"
                )

                if not items:
                    print("âŒ å•†å“ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸï¼ˆAIä¿®å¾©å¾Œã‚‚0ä»¶ï¼‰ã€‚")
                    break

                print(f"âœ… {len(items)}ä»¶ã®å•†å“ã‚’æ¤œå‡º (Page {page_idx})")

                page_results = []
                for item in items:
                    if count >= limit: break
                    try:
                        title = self._get_text_with_healing(item, "title", "å•†å“åã®ãƒ†ã‚­ã‚¹ãƒˆã¾ãŸã¯ç”»åƒã®altå±æ€§")
                        if title: title = title.replace("ã®ã‚µãƒ ãƒã‚¤ãƒ«", "").strip()
                        price = self._get_text_with_healing(item, "price", "å•†å“ã®ä¾¡æ ¼ï¼ˆæ•°å­—ã‚’å«ã‚€è¦ç´ ï¼‰")
                        
                        try: img_src = item.locator("img").first.get_attribute("src")
                        except: img_src = ""
                        try: href = item.locator("a").first.get_attribute("href")
                        except: href = ""
                        product_url = f"https://jp.mercari.com{href}" if href else ""

                        title = title or "å–å¾—å¤±æ•—"
                        price = price or "0"

                        img_filename = ""
                        if img_src:
                            try:
                                img_data = requests.get(img_src, timeout=5).content
                                safe_name = f"{count}_{int(time.time())}.jpg"
                                img_path = os.path.join(IMAGE_DIR, safe_name)
                                with open(img_path, "wb") as f: f.write(img_data)
                                img_filename = safe_name
                            except: pass

                        row = {"å•†å“å": title, "ä¾¡æ ¼": price, "ç”»åƒãƒ‘ã‚¹": img_filename, "URL": product_url}
                        page_results.append(row)
                        results.append(row)
                        count += 1
                        progress(count / limit, desc=f"å–å¾—ä¸­... {count}/{limit}ä»¶")
                    except Exception: continue
                
                if page_results:
                    pd.DataFrame(page_results).to_csv(csv_filename, mode='a', header=False, index=False, encoding="utf-8-sig")
                
                page_idx += 1
                if len(items) == 0: break

            browser.close()
            
        return f"å®Œäº†ï¼ {len(results)}ä»¶å–å¾—ã—ã¾ã—ãŸã€‚\nãƒ•ã‚¡ã‚¤ãƒ«: {csv_filename}", csv_filename

# --- Gradio UI ---
def start_scraping(api_key, keyword, category_name, limit, status, price_min, price_max):
    use_api_key = api_key if api_key else DEFAULT_API_KEY
    if not use_api_key: return "ã‚¨ãƒ©ãƒ¼: AIã®APIã‚­ãƒ¼ãŒå¿…è¦ã§ã™ã€‚", None
    scraper = MercariSmartScraper(use_api_key, DEFAULT_BASE_URL, DEFAULT_MODEL)
    cat_id = CATEGORY_MAP.get(category_name)
    return scraper.run(keyword, cat_id, status, price_min, price_max, int(limit))

with gr.Blocks() as demo:
    gr.Markdown("## ãƒ¡ãƒ«ã‚«ãƒªAIã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚° (å­¦ç¿’æ©Ÿèƒ½ä»˜ã)")
    gr.Markdown("AIãŒä¿®å¾©ã—ãŸã‚»ãƒ¬ã‚¯ã‚¿ã¯ `selectors.json` ã«ä¿å­˜ã•ã‚Œã€æ¬¡å›ã‹ã‚‰è‡ªå‹•çš„ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚")
    with gr.Accordion("APIè¨­å®š", open=False):
        api_key_input = gr.Textbox(label="API Key", type="password")
    with gr.Row():
        keyword_input = gr.Textbox(label="æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", value="ãƒ‹ãƒ³ãƒ†ãƒ³ãƒ‰ãƒ¼3DS")
        limit_input = gr.Number(label="ç›®æ¨™å–å¾—ä»¶æ•°", value=50, precision=0)
    with gr.Row():
        category_input = gr.Dropdown(label="ã‚«ãƒ†ã‚´ãƒª", choices=CATEGORY_CHOICES)
        status_input = gr.Dropdown(label="çŠ¶æ…‹", choices=["è²©å£²ä¸­", "å£²ã‚Šåˆ‡ã‚Œ", "ã™ã¹ã¦"], value="è²©å£²ä¸­")
    with gr.Row():
        price_min_input = gr.Number(label="ä¾¡æ ¼ä¸‹é™")
        price_max_input = gr.Number(label="ä¾¡æ ¼ä¸Šé™")
    btn = gr.Button("é–‹å§‹", variant="primary")
    output_log = gr.Textbox(label="ãƒ­ã‚°")
    output_file = gr.File(label="CSV")
    btn.click(start_scraping, inputs=[api_key_input, keyword_input, category_input, limit_input, status_input, price_min_input, price_max_input], outputs=[output_log, output_file])

if __name__ == "__main__":
    demo.queue().launch()