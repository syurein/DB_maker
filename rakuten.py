import pandas as pd
import requests
import google.generativeai as genai
import time
import json
import random
import os
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor, as_completed

# ==========================================
# è¨­å®šã‚¨ãƒªã‚¢
# ==========================================
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# 1. GAS URLãƒªã‚¹ãƒˆ (è¤‡æ•°å¯)
GAS_MANAGER_URLS = [
    "https://script.google.com/macros/s/AKfycbz9QRefEYzM6P_WVNa5M1J_99Ak3RYNqbWfve61cLDwAUXHhwhgjfcpvR94BK18LbYD/exec",
    "https://script.google.com/macros/s/AKfycbw2qu9bdAQ70k3QozUzHUP6w3CQMZhR4BykMvmwpfloorz5UqlpeqVaOESgJ9SAnACi/exec",
    "https://script.google.com/macros/s/AKfycbwFSy4pEVeGdue98Ps6q3V4_L2I0gJP9A5wanoW7eKKWbTZKPdImRLJJHvJNQ0bl28V/exec"
]

# 2. å…¥å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
# â€»ã‚«ãƒ©ãƒ åã¯å…¥åŠ›CSVã«åˆã‚ã›ã¦ã‚³ãƒ¼ãƒ‰å†…ã® row[...] éƒ¨åˆ†ã‚’èª¿æ•´ã—ã¦ãã ã•ã„
INPUT_CSV = "./merged_data_total_6542.csv" 

OUTPUT_CSV = "results_flat_data.csv"            # çµæœCSV (1è¡Œ1å•†å“)
HISTORY_LOG_FILE = "processed_history.log"      # å±¥æ­´ä¿å­˜ç”¨ãƒ•ã‚¡ã‚¤ãƒ«

# 3. å‹•ä½œè¨­å®š
REGEX_PATTERN = ".*"  # ãƒ•ã‚£ãƒ«ã‚¿ç”¨ (å…¨ä»¶ãªã‚‰ ".*")
MAX_WORKERS = 9     # ä¸¦åˆ—æ•°
SAVE_INTERVAL = 10    # ä½•å•†å“ã”ã¨ã«ä¿å­˜ã™ã‚‹ã‹

# ==========================================
# æº–å‚™
# ==========================================
genai.configure(api_key=GOOGLE_API_KEY)
model = genai.GenerativeModel('gemini-2.0-flash')

def load_history():
    """å±¥æ­´ãƒ­ã‚°ã‚’èª­ã¿è¾¼ã¿ã€å‡¦ç†æ¸ˆã¿ã®å•†å“åã‚»ãƒƒãƒˆã‚’è¿”ã™"""
    if not os.path.exists(HISTORY_LOG_FILE):
        return set()
    with open(HISTORY_LOG_FILE, 'r', encoding='utf-8') as f:
        # æ”¹è¡Œã‚’é™¤å»ã—ã¦ã‚»ãƒƒãƒˆã«æ ¼ç´
        return set(line.strip() for line in f if line.strip())

def append_history(product_names):
    """å‡¦ç†ã—ãŸå•†å“åã‚’ãƒ­ã‚°ã«è¿½è¨˜"""
    with open(HISTORY_LOG_FILE, 'a', encoding='utf-8') as f:
        for name in product_names:
            f.write(f"{name}\n")

def generate_search_keyword(product_name):
    """AIã§æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ"""
    prompt = f"""
    ä»¥ä¸‹ã®å•†å“åã‹ã‚‰ã€æ¥½å¤©å¸‚å ´ã§ä¾¡æ ¼èª¿æŸ»ã‚’ã™ã‚‹ãŸã‚ã®ã€Œæœ€ã‚‚ç²¾åº¦ã®é«˜ã„æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ã‚’1ã¤ã ã‘æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
    ã§ãã‚‹ã ã‘çŸ­ã„ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ãƒ’ãƒƒãƒˆæ•°ãŒå¤šããªã‚‹ã‚ˆã†ãªå˜èªã«ã—ã¦ãã ã•ã„ã€‚
    å‹ç•ªãŒã‚ã‚‹å ´åˆã¯å¿…ãšå«ã‚ã€è¡¨è¨˜ã‚†ã‚Œã‚’ãªãã—ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªèª¬æ˜ã¯ä¸è¦ã§ã™ã€‚
    
    å•†å“å: {product_name}
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:
    """
    try:
        time.sleep(random.uniform(0.5, 1.5))
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception:
        return product_name

def fetch_data_from_gas(keyword):
    """GASã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—"""
    target_url = random.choice(GAS_MANAGER_URLS)
    try:
        # APIåˆ¶é™ã‚„ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼å¯¾ç­–ã§ãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ã‚’å…¥ã‚Œã‚‹ã¨ãªãŠè‰¯ã—
        response = requests.get(target_url, params={"q": keyword}, timeout=45)
        data = response.json()
        return data.get("prices", [])
    except Exception as e:
        print(f"âŒ GAS Error ({keyword}): {e}")
        return []

def process_single_row_task(row):
    """
    ä¸¦åˆ—å‡¦ç†ç”¨ã®ã‚¿ã‚¹ã‚¯é–¢æ•°
    1ã¤ã®å…¥åŠ›è¡Œã«å¯¾ã—ã€è¤‡æ•°ã®çµæœè¡Œï¼ˆãƒªã‚¹ãƒˆï¼‰ã‚’è¿”ã™
    """
    # åˆ—åã®æºã‚‰ãå¸å
    original_name = row.get('product_name') or row.get('å•†å“å')
    
    # 1. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç”Ÿæˆ
    search_keyword = generate_search_keyword(original_name)
    
    # 2. æ¥½å¤©ãƒ‡ãƒ¼ã‚¿å–å¾—
    items_list = fetch_data_from_gas(search_keyword)
    
    result_rows = []
    
    # 3. ãƒ’ãƒƒãƒˆã—ãŸå•†å“ã‚’1ã¤ãšã¤è¡Œã«ã™ã‚‹
    if items_list:
        for item in items_list:
            result_rows.append({
                "product_name": item.get('name'),       # æ¥½å¤©ã®å•†å“å
                "price": item.get('price'),             # ä¾¡æ ¼
                "image_url": item.get('image_url'),     # ç”»åƒURL
                "item_url": item.get('url'),            # å•†å“URL
                "data_source": "Rakuten"
            })
        print(f"âœ… Hit: {search_keyword} -> {len(items_list)}ä»¶")
    else:
        # ãƒ’ãƒƒãƒˆã—ãªã‹ã£ãŸå ´åˆã€CSVç”¨ã®ãƒªã‚¹ãƒˆ(result_rows)ã«ã¯ä½•ã‚‚è¿½åŠ ã—ãªã„
        # ã“ã‚Œã«ã‚ˆã‚Šã€CSVã«ã¯æ›¸ãè¾¼ã¾ã‚Œãªã„ãŒã€original_nameã¯è¿”ã•ã‚Œã‚‹ã®ã§å±¥æ­´ã«ã¯æ®‹ã‚‹
        print(f"âš ï¸ No Hit: {search_keyword} (ãƒ­ã‚°ã®ã¿è¨˜éŒ²)")

    return original_name, result_rows

# ==========================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ==========================================
def main():
    # 1. å…¥åŠ›CSVèª­ã¿è¾¼ã¿
    try:
        df = pd.read_csv(INPUT_CSV)
    except FileNotFoundError:
        print(f"âŒ Error: {INPUT_CSV} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return

    # 2. å±¥æ­´èª­ã¿è¾¼ã¿ & ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    processed_history = load_history()
    print(f"ğŸ“œ History loaded: {len(processed_history)} items processed.")

    # å‡¦ç†å¯¾è±¡ã®ã‚«ãƒ©ãƒ ç‰¹å®š
    target_col = 'product_name' if 'product_name' in df.columns else 'å•†å“å'
    
    # ãƒ•ã‚£ãƒ«ã‚¿: æ­£è¦è¡¨ç¾ AND æœªå‡¦ç†ã®ã‚‚ã®
    if REGEX_PATTERN and REGEX_PATTERN != ".*":
        df = df[df[target_col].astype(str).str.contains(REGEX_PATTERN, regex=True, case=False, na=False)]
    
    # å±¥æ­´ã«ã‚ã‚‹ã‚‚ã®ã¯é™¤å¤–
    df_target = df[~df[target_col].isin(processed_history)]
    
    total_targets = len(df_target)
    print(f"ğŸ‘‰ Processing targets: {total_targets} items (Skipped: {len(df) - total_targets})")
    
    if total_targets == 0:
        print("ğŸ‰ å…¨ã¦å‡¦ç†æ¸ˆã¿ã§ã™ï¼")
        return

    all_results = []       # çµæœãƒ‡ãƒ¼ã‚¿ã‚’æºœã‚ã‚‹ãƒªã‚¹ãƒˆ
    just_processed = []    # ä»Šå›ã®ãƒãƒƒãƒã§å‡¦ç†å®Œäº†ã—ãŸå•†å“åãƒªã‚¹ãƒˆ

    # 3. ä¸¦åˆ—å‡¦ç†é–‹å§‹
    print(f"ğŸš€ Starting processing...")
    
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # ã‚¿ã‚¹ã‚¯ç™»éŒ²
        future_to_row = {executor.submit(process_single_row_task, row): row for _, row in df_target.iterrows()}
        
        completed_count = 0
        
        for future in as_completed(future_to_row):
            try:
                # çµæœã‚’å—ã‘å–ã‚‹ (å…ƒã®å•†å“å, çµæœã®ãƒªã‚¹ãƒˆ)
                orig_name, rows = future.result()
                
                # rowsãŒç©ºï¼ˆNo Hitï¼‰ã®å ´åˆã¯ãƒªã‚¹ãƒˆã«è¿½åŠ ã•ã‚Œãªã„
                if rows:
                    all_results.extend(rows)
                
                # æ¤œç´¢è‡ªä½“ã¯å®Œäº†ã—ãŸã®ã§å±¥æ­´ãƒªã‚¹ãƒˆã«ã¯è¿½åŠ ã™ã‚‹
                just_processed.append(orig_name)
                
                completed_count += 1
                
                # 4. å®šæœŸä¿å­˜
                if completed_count % SAVE_INTERVAL == 0:
                    print(f"ğŸ’¾ Saving chunk... ({completed_count}/{total_targets})")
                    
                    # æ›¸ãè¾¼ã‚€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿ä¿å­˜å‡¦ç†ã‚’è¡Œã†
                    if all_results:
                        output_df = pd.DataFrame(all_results)
                        
                        # åˆå›ä½œæˆæ™‚ã¨è¿½è¨˜æ™‚ã®ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                        if os.path.exists(OUTPUT_CSV):
                            output_df.to_csv(OUTPUT_CSV, mode='a', header=False, index=False, encoding='utf-8-sig')
                        else:
                            output_df.to_csv(OUTPUT_CSV, mode='w', header=True, index=False, encoding='utf-8-sig')
                        
                        # ãƒ¡ãƒ¢ãƒªè§£æ”¾ã®ãŸã‚ãƒªã‚¹ãƒˆã‚’ã‚¯ãƒªã‚¢
                        all_results = [] 
                    else:
                        print("  (No valid hits in this chunk to save)")

                    # å±¥æ­´ãƒ­ã‚°ä¿å­˜ï¼ˆãƒ’ãƒƒãƒˆæœ‰ç„¡ã«é–¢ã‚ã‚‰ãšä¿å­˜ï¼‰
                    append_history(just_processed)
                    just_processed = [] # ã‚¯ãƒªã‚¢

            except Exception as e:
                print(f"âŒ Error in thread: {e}")

    # 5. æ®‹ã‚Šã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    if all_results:
        output_df = pd.DataFrame(all_results)
        if os.path.exists(OUTPUT_CSV):
            output_df.to_csv(OUTPUT_CSV, mode='a', header=False, index=False, encoding='utf-8-sig')
        else:
            output_df.to_csv(OUTPUT_CSV, mode='w', header=True, index=False, encoding='utf-8-sig')
    
    # æ®‹ã‚Šã®å±¥æ­´ã‚‚ä¿å­˜
    if just_processed:
        append_history(just_processed)

    print(f"\nğŸ‰ Process Complete! Log saved to {HISTORY_LOG_FILE}")

if __name__ == "__main__":
    main()